\section{Descrizione degli algoritmi}

\subsection{Struttura dati per il grafo}

La struttura dati per il grafo è stata implementata nel seguente
modo:
\begin{enumerate}
    \item \verb|V| è un insieme di nodi;
    \item \verb|E| è una lista di lati;
    \item \verb|graph| rappresenta la \textit{lista di adiacenza}.
    Gli indici per accedere alla mappa sono rappresentati dai vertici.
    Ogni cella della mappa punta ad una lista di coppie di valori
    (il vertice a cui è collegato e il peso del lato che li
    congiunge).
\end{enumerate}

\noindent I metodi implementati sono:
\begin{enumerate}
    \item \verb|add_vertex|: aggiunge un vertice al grafo;
    \item \verb|add_edge|: aggiunge un lato al grafo;
    \item \verb|remove_edge|: rimuove un lato dal grafo.
\end{enumerate}
Queste operazioni sono state implementate in modo da avere una
complessità computazionale costante.

\subsection{Kruskal naive}

\subsubsection{Introduzione}

La versione naive dell'algoritmo di Kruskal ha una complessità
computazionale pari a $O(mn)$. L'idea alla base di questo
algoritmo è quella di ordinare i lati in ordine crescente rispetto al
loro peso $w$. Una volta ordinati, per ogni lato si controlla se
aggiungendolo al grafo temporaneo questo crei un ciclo. Se crea un
ciclo allora tale lato non verrà inserito, altrimenti il lato farà
parte del MST $T$.

Ordinando i lati in base al loro peso e controllando che l'inserimento
di un lato non crei un ciclo nel grafo, si otterrà un albero la cui
somma dei suoi lati sarà minima.

\noindent Si illustra lo pseudo-codice:
\begin{verbatim}
    Kruskal-Naive(G)
        A = Graph()
        for each vertex v in G.V
            G.add_vertex(v)
        sort the edges of G.E into increasing order by weight w
        for each edge (u, v) in G.E, taken in increasing order by weight
            if A U {(u, v)} is acyclic
                A = A U {(u, v)}
        return A
\end{verbatim}

\subsubsection{Implementazione}

Nella nostra implementazione abbiamo utilizzato l'algoritmo di
ordinamento \textit{merge sort}, che ha una complessità
computazionale pari a $\Theta(n log(n))$, per ordinare i lati
in ordine crescente rispetto al loro peso.

Il Minimum Spanning Tree viene salvato in una struttura dati
\verb|Graph|; il MST può essere esaminato andando ad
esplorare la lista di adiacenza \verb|graph| o la lista dei
lati \verb|E|.

Il controllo per la ciclicità viene implementato con una
versione modificata della \textit{Depth First Search}
(\textit{DFS}).

\subsubsection{Ottimizzazioni implementate}

Per ottimizzare l'algoritmo abbiamo apportato delle opportune
modifiche. A partire dal metodo \verb|kruskal_naive|, ad
ogni iterazione del ciclo si controlla se il numero di lati
del grafo $A$ (quello che conterrà il MST) ha $n - 1$ lati,
dove $n$ è il numero di vertici del grafo $G$, fornito in input.
Quando si raggiunge questa uguaglianza significa che è stato
costruito il MST $T$, pertanto l'algoritmo può terminare.
Questa prima modifica permette di evitare di fare iterazioni
inutili che non contribuiranno alla costruzione del MST.

La seconda ottimizzazione implementata riguarda il metodo
\verb|_is_acyclic|. Un primo controllo riguarda i vertici
connessi dal lato: se i due vertici sono uguali significa
che il lato in questione crea un \textit{self-loop}. Con
questo controllo siamo in grado eliminare i \textit{self-loop} in
tempo costante, senza dover effettuare una chiamata a DFS per verificare
che l'aggiunta di tale lato crei un ciclo nel grafo $A$.
Il secondo controllo che è stato implementato in questo metodo
riguarda i vertici $u$ e $v$ che il lato, fornito in input, connette.
Se almeno uno dei due lati non è presente nel grafo $A$, allora
possiamo concludere che tale lato non introdurrà un ciclo nel grafo,
in quanto il lato in questione permetterà di scoprire almeno un nuovo
vertice del MST finale. Questa accortezza ci permette di risparmiare
delle chiamate a DFS soprattutto nella fase di avvio
dell'algoritmo. Se invece, sia $u$ che $v$ sono già presenti
nel grafo $A$, allora è necessario fare una chiamata a DFS per verificare
che quel lato non introduca un ciclo nel grafo.

La terza ottimizzazione riguarda DFS: il metodo che è stato implementato
non rispecchia la versione originale dell'algoritmo. Lo scopo di DFS
è quello di andare a visitare \textit{tutti} i vertici di un certo
grafo; tuttavia, questo non rappresenta il nostro scopo. Il nostro
intento è quello di cercare, se esiste, un cammino che congiunge $u$ con
$v$. Se tali vertici si trovano in due componenti connesse distinte
(ad esempio nel momento in cui si stanno aggiungendo i lati al
grafo $A$), significa che l'aggiunta del lato $(u, v)$ al grafo $A$ non
introdurrà un ciclo. Avviando DFS dal vertice $u$ sul grafo $A$,
desidero verificare se esiste un cammino che mi porta fino a $v$.
Se entrambi i vertici sono nella stessa componente connessa, allora
tale cammino verrà trovato, altrimenti tale cammino non verrà
trovato perché non esiste. Quindi, DFS verrà eseguita soltanto sulla
componente connessa di $u$ e non su tutti i vertici del grafo.
Questa versione permette di risparmiare la visita di tutto il grafo
e la sua complessità computazionale diventa $O(m)$.

Queste tre ottimizzazioni permettono di evitare di fare eccessive
chiamate a DFS e di evitare di andare a visitare l'intero grafo
anche quando un cammino da $u$ a $v$ non esiste. La complessità
computazionale rimane sempre $O(mn)$, però l'esecuzione è sensibilmente
molto più veloce rispetto ad un'implementazione senza queste accortezze.

Il metodo \verb|is_there_a_path| serve soltanto per inizializzare
il vettore di booleani che indica se un vertice è stato visitato o meno.
Tale vettore è necessario a DFS per esplorare il grafo (o la
componente connessa in cui vi è $u$).

\subsection{Kruskal con Union-Find}

\subsubsection{Introduzione}

La versione naive dell'algoritmo di Kruskal svolge ripetutamente
l'operazione di controllo di aciclicità del grafo $A$. Questo continuo
controllo è responsabile della complessità computazionale
dell'algoritmo. Per ottimizzare le prestazioni, il controllo della
ciclicità del grafo viene catturato tramite l'implementazione di
una particolare struttura dati: gli \textbf{insiemi disgiunti}
(o \textbf{disjoint sets}).

\subsubsection{Struttura dati}

La struttura dati per gli insiemi disgiunti è stata implementata
come un \textit{albero}. Inizialmente, ogni vertice viene
rappresentato come un albero con un solo nodo, il cui \textit{parent}
è sè stesso. Man mano che si vanno a visitare i vari lati del
grafo $G$, si controlla se i due vertici, connessi al lato in questione,
appartengono allo stesso albero o meno. Se appartengono allo stesso
albero significa che hanno lo stesso nodo radice, se invece appartengono
ad alberi differenti allora avranno radici diverse. Per
l'implementazione di questo algoritmo non è importante né l'ordine dei
nodi salvati nell'albero, né quale sia il nodo alla radice dell'albero.
Se i vertici del lato $(u, v)$ appartengono a due alberi diversi,
allora si procede ad eseguire una $union$ dei due alberi. Questa
operazione è stata implementata come una \textit{union-by-size}.
Per poter implementare tale operazione nella struttura dati si tiene
conto anche della \textit{size} di ogni albero: ogni nodo memorizza
la sua dimensione, che rappresenta il numero di discendenti (incluso
sè stesso). Nel momento in cui bisogna unire i due alberi, l'albero
con la dimensione maggiore diventa genitore dell'altro; nel caso in
cui i due alberi abbiano la stessa dimensione, allora la scelta
diventa casuale, a meno che non debbano essere mantenute determinate
proprietà (non è questo il nostro caso).

I metodi implementati per tale struttura dati sono:
\begin{enumerate}
    \item \verb|make_set(x)|: crea un albero costituito da un solo nodo
    il cui valore è $x$, la \textit{size} è impostata a 1 e il
    \textit{parent} è sè stesso. Complessità computazionale: costante;
    \item \verb|find_set(x)|: ritorna il nodo radice dell'albero a cui
    appartiene il nodo che ha valore $x$. Complessità computazionale:
    $O(log n)$;
    \item \verb|union_by_size(x, y)|: dati i valori di due nodi, vengono
    determinati i nodi radice per ciascuno e se appartengono ad alberi diversi
    allora si procede nell'unire i due alberi, secondo le procedure definite
    precedentemente. Complessità computazionale: $O(log(n))$.
\end{enumerate}

\subsubsection{Implementazione}

Lo pseudo-codice non si discosta molto dalla versione naive dell'algoritmo.
Le uniche parti che variano sono:
\begin{enumerate}
    \item $A$ non è più un grafo, ma è una collezione di lati;
    \item Non è più richiesto effettuare il controllo che il grafo sia
    aciclico.
\end{enumerate}

Si illustra lo pseudo-codice:
\begin{verbatim}
    Kruskal-Union-Find(G)
        A = empty_set
        for each vertex v in G.V
            make-set(v)
        sort the edges of G.E into increasing order by weight w
        for each edge (u, v) in G.E, taken in increasing order by weight
            if find-set(u) != find-set(v)
                A = A U {(u, v)}
                union(u, v)
        return A
\end{verbatim}

Il controllo per la ciclicità del grafo viene sostituito con la verifica
che i due vertici del lato $(u, v)$ appartengano o meno a due alberi
distinti. Se i due vertici hanno il nodo radice in comune significa che
esiste già un percorso da $u$ a $v$ e quindi aggiungendo il lato $(u, v)$
si creerebbe un ciclo. Se invece i due vertici appartengono a due
alberi diversi, allora l'introduzione di tale arco non creerà un
ciclo nel grafo, ma permetterà di scoprire dei nuovi vertici del MST
$T$ finale. Quindi, il lato $(u, v)$ verrà aggiunto ad $A$ e i due
alberi verranno uniti in base alla loro \textit{size}.

L'introduzione degli insiemi disgiunti riduce la complessità computazionale
a $O(m log(n))$. Analizzando l'algoritmo, si può osservare che:
\begin{enumerate}
    \item vengono svolte $n$ chiamate a \verb|make_set(x)|;
    \item vengono ordinati i lati in ordine crescente con \textit{merge sort},
    quindi si ha una complessità pari a $\Theta(m log(m))$;
    \item il ciclo for ha una complessità pari a $O((n + n)\alpha(n)))$,
    dove $\alpha$ è una funzione che cresce molto lentamente (è l'inversa
    della funzione di Ackermann). Assumendo che $G$ sia connesso, le
    operazioni per gli insiemi disgiunti hanno una complessità pari a
    $O(m\alpha(n))$. Inoltre, possiamo dire che
    $\alpha(n) = O(log(n)) = O(log(m))$, quindi la complessità dell'algoritmo
    nel suo complesso diventerebbe $O(m log(m))$.

    Si può osservare però che i grafi rispettano la diseguaglianza $m < n^2$.
    Quindi, $log(m) = O(log(n))$. Con questa osservazione, la complessità
    finale dell'algoritmo è $O(m log(n))$.
\end{enumerate}

\subsection{Prim}

\subsubsection{Introduzione}

La versione base dell'algoritmo di Prim ha una complessità computazionale
pari a $\mathcal{O}(mn)$. L'idea alla base di questo algoritmo è quella di partire da un
nodo arbitrario e scegliere, a ogni iterazione, l'arco che connetta con il minor peso possibile
l'albero dei nodi del \textit{minimum spanning tree} al nuovo vertice. \\
Nonostante la complessità sia polinomiale, e quindi efficiente, l'algoritmo può essere ottimizzato
utilizzando la corretta struttura dati: l'implementazione dell'algoritmo di Prim con l'utilizzo di
\textit{Heap}, infatti, permette di calcolare il minimo nodo da aggiungere all'albero in tempo logaritmico;
la complessità computazionale dell'algoritmo in questa implementazione, quindi, diventa \newline $\mathcal{O}(m+n log(n))$. \\
Si illustra lo pseudocodice dell'algoritmo di Prim con l'utilizzo di \textit{Heap}:
\newpage
\begin{verbatim}
    Prim(G,s)
        for each u in V do
            key[u] <- +inf
            parent[u] <- nil
        key[s] <- 0
        Q <- V
        while Q not empty do
            u <- extractMin(Q)
            for each v adjacent to u do
                if v in Q and w(u,v) < key[v] then
                    parent[v] <- u
                    key[v] <- w(u,v)
\end{verbatim}

\subsubsection{Struttura dati}

Per la memorizzazione dei vertici del grafo di partenza, e per estrarne successivamente il nodo di peso minimo, abbiamo creato una classe \texttt{Heap} con i seguenti campi dati:
\begin{itemize}
    \item \texttt{list}, ossia una lista contenente i nodi del grafo rappresentati da oggetti di tipo \texttt{Node};
    \item \texttt{mapList}, ossia una mappa che permette di associare un nodo alla sua posizione in \texttt{list}. L'utilità di questa variabile viene discussa nel capitolo successivo;
    \item \texttt{currentSize}, ossia la dimensione del grafo.
\end{itemize}
La classe \texttt{Heap} mette a disposizione i metodi classici della struttura dati \textit{Heap}; particolarmente importanti ai fini del suo utilizzo nell'algoritmo sono i seguenti metodi:
\begin{itemize}
    \item \texttt{search} permette di sapere se un nodo è presente o meno nel grafo, ed è di particolare importanza per il controllo dell'esistenza di un nodo durante l'esecuzione dell'algoritmo;
    \item \texttt{extractMin} permette di estrarre il nodo di peso minimo in tempo logaritmico;
    \item \texttt{searchAndUpdateWeight} permette di aggiornare il peso di un nodo, ed è necessario
    per l'aggiornamento del grafo in seguito alla rimozione del nodo in testa al ciclo \textit{while}.
\end{itemize}

\subsubsection{Implementazione}
La nostra implementazione ricalca l'implementazione standard dell'algoritmo di Prim con l'utilizzo di un 
\textit{MinHeap} come struttura dati a supporto; essa non si discosta quindi dallo pseudocodice precedentemente mostrato.


\subsubsection{Ottimizzazioni implementate}

Anche per quanto riguarda l'algoritmo di Prim abbiamo apportato una opportuna modifica per ottimizzarne l'esecuzione. \\
Tale ottimizzazione consiste nell'utilizzo di una mappa, concretizzata in una variabile di tipo
\texttt{defaultdict} in \textit{Python}, che associa a ogni nodo la sua posizione in \texttt{list}, ossia
la lista dei nodi. Questa implementazione permette di ricercare la presenza di un vertice nello \textit{Heap} in tempo costante,
e ciò è particolarmente utile perché la presenza del vertice deve essere verificata due volte ad ogni iterazione del ciclo \textit{for}
dell'algoritmo (nella condizione \textit{if} e durante l'\textit{update} dello \textit{Heap}).
Se non avessimo adottato tale ottimizzazione, queste operazioni avrebbero avuto una complessità lineare, andando così ad aumentare
polinomialmente la complessità computazionale dell'algoritmo.