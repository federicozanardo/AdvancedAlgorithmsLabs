\section{Descrizione degli algoritmi}

\subsection{Struttura dati per il grafo}

\subsubsection{Introduzione}

Il TSP prende in input un grafo \textit{completo}, ovvero, un grafo in cui:
\[
    \forall u, v \in V \textnormal{ } \exists (u, v) \in E
\]
Il grafo in questione viene definito \textit{denso} e la stuttura dati indicata per rappresentarlo è
la \textit{matrice di adiacenza}, di dimensione $|V| \times |V|$. Le celle della matrice contengono
i pesi dei lati che congiungono i due vertici.

La struttura dati per il grafo è stata implementata nel seguente
modo:

\subsubsection{Implementazione}

\subsection{Held e Karp}

\subsubsection{Introduzione}

\subsubsection{Implementazione}

\subsubsection{Ottimizzazioni implementate}




\subsection{Nearest Neighbor}

\subsubsection{Introduzione}

L'algoritmo \textit{Nearest Neighbor} per il problema del Traveling Salesman Problem rientra nelle cosidette \textit{euristiche costruttive}; queste sono una famiglia di algoritmi che, a partire da un vertice iniziale, procedono aggiungendo un vertice alla volta al sottoinsieme corrispondente alla soluzione parziale. Questa aggiunta viene eseguita secondo regole prefissate, fino all'individuazione di una soluzione approssimata.

\subsubsection{Algoritmo}

Come tutte le euristiche costruttive per il problema TSP, l'algoritmo \textit{nearest neighbor} può essere scomposto in tre fasi: inizializzazione, selezione, inserimento. Viene qui illustrato l'algoritmo in pseudocodice, e di seguito vengono analizzate nello specifico le tre fasi.

\begin{verbatim}
    NearestNeighbor(G=(V,E))
    # il cammino iniziale è composto unicamente dal primo nodo del grafo
    path <- v1 in V

    # si itera finché tutti i nodi del grafo non sono stati inseriti nel path
    while path != V 
        # si seleziona il nodo di distanza minima dall'ultimo nodo inserito nel path
        nextNode <- MinAdj(path, path.lastV)

        # si inserisce il nodo successivo nel path
        path <- nextNode
    
    # si aggiunge infine il nodo di partenza per chiudere il ciclo
    path <- path + v1

    return path


    MinAdj(G=(V,E), path, lastV)
        return minimum distance node from lastV in {E-path}

\end{verbatim}

Le tre fasi quindi sono:
\begin{enumerate}
    \item \textbf{Inizializzazione}: si parte con il cammino composto unicamente dal primo vertice;
    \item \textbf{Selezione}: sia $(v_0,...,v_k)$ il cammino corrente; il vertice successivo è il vertice $v_{k+1}$, non presente nel cammino, a distanza minima da $v_k$;
    \item \textbf{Inserimento}: viene inserito $v_{k+1}$ subito dopo $v_k$ nel cammino.
\end{enumerate}

Le fasi 2 e 3 vengono ripetute finché tutti i vertici non sono nel cammino finale. Una volta verificata questa condizione è necessario eseguire un'ulteriore singola istruzione: aggiungere in coda al percorso finale il nodo iniziale. Questa operazione è infatti necessaria per definizione del problema.

\subsubsection{Analisi dei fattori di approssimazione}

Definiamo la disuguaglianza triangolare come:
\begin{equation}
    \forall u, v, w \in V, \textnormal{ vale che } c(u, v) \le c(u, w) + c(w, v)
\end{equation}

L'algoritmo \textit{nearest neighbor} permette di trovare una soluzione $log(n)$-approssimata a TSP quando la disuguaglianza triangolare è rispettata.

\subsubsection{Implementazione}

\subsubsection{Ottimizzazioni implementate}





\subsection{2-approssimato}

\subsubsection{Introduzione}

L'algortimo 2-approssimato è in grado di determinare un'approssimazione del TSP sotto la condizione che le
distanze rispettino la \textit{disuguaglianza triangolare}, ovvero:
\begin{equation}
    \forall u, v, w \in V, \textnormal{ vale che } c(u, v) \le c(u, w) + c(w, v)
\end{equation}

dove $c$ è la \textit{funzione di costo}. Questo implica che il cammino con un lato $c(u, v)$ ha un costo
minore o uguale al costo del cammino con due lati $c(u, w, v)$. Il nome di questo problema è \textbf{TRIANGLE\_TSP}.

\subsubsection{Definizione di MST}

Sia $V$ l'insieme dei nodi che costituiscono il grafo pesato $G$ e sia $E$ la collezione dei lati di tale
grafo. Ai fini delle analisi della complessità degli algoritmi, sia $|V| = n$ e $|E| = m$.

Un \textit{minimum spanning tree} è un
sottoinsieme dei lati $E$ di un grafo $G$ non orientato connesso e pesato sui lati che
collega tutti i vertici insieme, senza alcun ciclo e con il minimo peso totale del
lato possibile. Cioè, è uno spanning tree la cui somma dei pesi dei bordi è la più
piccola possibile.

Un \textit{minimum spanning tree} $T = (V, E')$ è un albero, il cui insieme dei lati $E'$ è un
sottoinsieme dei lati $E$ di un grafo $G = (V, E)$ non orientato, connesso e
pesato, che collega tutti i vertici $V$, la cui somma dei pesi dei lati è la minima.

L'algoritmo generico per determinare un MST è:
\begin{verbatim}
    A = empty_set
    while A doesn't form a spanning tree
        find an edge (u,v) that is safe for A
        A = A U {(u,v)}
    return A
\end{verbatim}

Si forniscono alcune definizioni per gli MST:
\begin{enumerate}
    \item un \textbf{taglio} $(S, V \setminus S)$ di un grafo $G = (V, E)$ è una partizione
    di $V$;
    \item un lato $(u, v) \in E$ \textbf{attraversa il taglio} $(S, V \setminus S)$ se
    $u \in S$ e $v \in V \setminus S$ o viceversa;
    \item un taglio \textbf{rispetta} un insieme $A$ di lati se nessun lato di $A$ attraversa
    il taglio;
    \item dato un taglio, il lato che lo attraversa di peso minimo si chiama \textbf{light edge}.
\end{enumerate}

Per determinare se un lato è \textbf{safe}, si sfrutta il seguente teorema:

\textbf{Teorema}: Sia $G = (V, E)$ un grafo non diretto, connesso e pesato. Sia $A$ un
sottoinsieme di $E$ incluso in una qualche MST di $G$, sia $(S, V \setminus S)$ un
taglio che rispetta $A$, e sia $(u, v)$ un \textit{light edge} per $(S, V \setminus S)$.
Allora $(u, v)$ è \textit{safe} per $A$.

\subsubsection{Algoritmo}

L'idea che sta dietro a questo algoritmo consiste nell'utilizzare un algoritmo per il calcolo
del MST. Tuttavia, il MST è un albero e quello che vogliamo ottenere invece è un
ciclo hamiltoniano. Per fare ciò, eseguiamo una visita in \textit{preorder} del MST
e aggiungiamo la radice di tale MST alla lista della determinata dalla preorder. Questo è un ciclo
hamiltoniano del grafo originale, in quanto quest'ultimo è completo. Pertanto, esiste sempre un
lato tra ogni coppia di vertici.

\begin{verbatim}
    2-APPROXIMATION(G=(V,E), c)
        root <- v1 in V         // scelgo un nodo di V come radice per Prim
        T <- Prim(G, c, root)
        H' <- preorder(root)    // visita in preorder
        H <- H U {root}           // aggiungo il percorso che va dalll'ultimo nodo
                                // della visita in preorder alla radice
        return H

\end{verbatim}

\subsubsection{Analisi della qualità della soluzione}

Si illustri l'analisi della qualità della soluzione ritornata dall'algoritmo:
\begin{enumerate}
    \item Il costo di $H'$ è basso per la definizione di MST;
    \item supponiamo che presi due vertici $a$ e $b$ non siano collegati da un lato (anche se
    sappiamo che il grafo è completo), L'idea è che il costo di $(a, b)$ è minore rispetto
    al costo di fare un giro più largo, in quanto vale la \textit{disuguaglianza triangolare}.
    Quindi in questo caso $(a, b)$ è un \textit{shortcut}.
\end{enumerate}

\subsubsection{Analisi del fattore di approssimazione}
Si illustri l'analisi del fattore di approssimazione dell'algoritmo:
\begin{enumerate}
    \item \textit{Limite inferiore al costo della soluzione ottima $H$}: sia $H$ il ciclo
    ottimo $H^{ottimo} = <v_{j1}, ..., v_{jn}, v_{j1}>$.
    Sia $H'^{ottimo} = <v_{j1}, ..., v_{jn}>$ un cammino
    (è uno spanning tree, non un ciclo) hamiltoniano. Quindi, $c(H') \ge c(T)$, dove $T$ è
    il MST, e $c(H^{ottimo}) \ge c(H'^{ottimo})$ prechè i costi sono maggiori o uguali a zero.

    \item \textit{Limite superiore al costo della soluzione restituita $H$}: dato un albero,
    una \textit{full preorder chain} è una lista con ripetizioni dei nodi dell'albero che
    indica i nodi raggiunti dalle chiamate ricorsive dell'algoritmo \verb|Preorder|.

    \textit{Proprietà}: in una full preorder chain ogni arco di $T$ appare esattamente due
    volte. Quindi $c(\textnormal{full preorder chain}) = 2 \cdot c(T)$. Se si eliminano
    dalla full preorder chain tutte le occorrenze successive alla prima dei nodi interni
    (tranne l'ultima occorrenza della radice) otteniamo, grazie alla
    \textit{disuguaglianza triangolare}:
    \[
        2 \cdot c(T) \ge c(H) \Rightarrow 2 \cdot c(H^{ottimo}) \ge 2 \cdot c(T) \ge c(H)
        \Rightarrow \frac{c(H)}{c(H^{ottimo})} \le 2
    \]

\end{enumerate}

\subsubsection{Implementazione}

\subsubsection{Ottimizzazioni implementate}

