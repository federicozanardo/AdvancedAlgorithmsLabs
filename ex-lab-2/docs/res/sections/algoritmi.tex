\section{Descrizione degli algoritmi}

\subsection{Struttura dati per il grafo}

\subsubsection{Introduzione}

Il TSP prende in input un grafo \textit{completo}, ovvero, un grafo in cui:
\[
    \forall u, v \in V \textnormal{ } \exists (u, v) \in E
\]
Il grafo in questione viene definito \textit{denso} e la struttura dati indicata per rappresentarlo è
la \textit{matrice di adiacenza}, di dimensione $|V| \times |V|$. Le celle della matrice contengono
i pesi dei lati che congiungono i due vertici.

\subsubsection{Implementazione}

La struttura dati per il grafo è rappresentata dalla classe \texttt{TSP}; questa possiede i seguenti campi dati:
\begin{itemize}
    \item \texttt{name}: il nome del dataset che un oggetto \texttt{TSP} rappresenta;
    \item \texttt{dimension}: la dimensione, ossia il numero di nodi, del dataset;
    \item \texttt{etype}: il tipo di rappresentazione delle coordinate dei vertici del dataset. Nel nostro programma questa assume uno dei due seguenti valori: \begin{itemize}
        \item \texttt{EUC\_2D} nel caso in cui le coordinate siano rappresentate in forma euclidea;
        \item \texttt{GEO} nel caso in cui le coordinate siano rappresentate da latitudine e longitudine;
    \end{itemize}
    \item \texttt{nodes}: struttura dati di tipo \texttt{defaultdict(list)}, ossia lista a dizionario. Questo campo dati contiene i vertici del grafo, indicizzati per indice riportato nel dataset;
    \item \texttt{adjMatrix}: matrice di adiacenza dei nodi. La matrice è di dimensione $|V|*|V|$, e conserva la seguente proprietà: \textit{Detti i,j gli indici di due nodi, adjMatrix[i][j] = adjMatrix[j][i] = w(i,j)}.
\end{itemize}

La classe presenta inoltre i seguenti metodi:
\begin{itemize}
    \item \texttt{add\_node}: aggiunge un vertice al grafo;
    \item \texttt{get\_weight}: restituisce il peso tra due vertici;
    \item \texttt{calculateAdjMatrix}: calcola la matrice di adiacenza del grafo. Questo metodo viene chiamato una sola volta, appena sono stati caricati tutti i metodi;
    \item \texttt{delete\_node}: elimina un nodo dal grafo;
    \item \texttt{get\_min\_node}: restituisce il nodo più vicino al nodo che viene dato in \textit{input}. Tale metodo è particolarmente utile per l'implementazione dell'algoritmo \texttt{nearest neighbor}, come specificato in seguito;
    \item \texttt{printAdjMatrix}: funzione di utilià per la stampa della matrice di adiacenza. 
\end{itemize}

\subsection{Held e Karp}

\subsubsection{Introduzione}

L'algoritmo di Held and Karp è un algoritmo di programmazione dinamica che permette di determinare in modo esatto il risultato di TSP sfruttando la scomposizione del problema in sotto-problemi e in sotto-sotto-problemi che vengono ricorsivamente risolti. Per ciascun sotto-problema viene generato un risultato che viene salvato all'interno di una tabella contenente tutte le soluzioni che portano poi alla determinazione del cammino minimo di TSP.

L'algoritmo si basa sulla proprietà secondo la quale ogni sottocammino di un cammino minimo è a sua volta un cammino minimo, e ciò ci permette di calcolare i sottoproblemi ricorsivamente nel seguente modo: 

\begin{itemize}
    \item  pongo \(1 \dots n\) città (i.e. nodi) da visitare;
    \item  pongo \(S \subseteq V\) e \(v \in S\), dove \(V\) sono le tutte le città in esame, \(S\) le città visitate fino a quel momento e \(v\) la città corrente;

\item definisco \(d[v, S]\) peso del cammino minimo nella città \(v\) a partire dalla città 1 percorrendo \(S\) città;
\item definisco \(\pi[v, S]\) predecessore di \(v\) nel cammino minimo percorrendo \(S\) città.
\end{itemize}

L'esecuzione dell'algoritmo a partire dalla città \(1\) visitando tutti i nodi \(S\) porterà al risultato del cammino minimo che sarà contenuto nel vettore \(d[1, S]\), ripercorribile poi con il vettore \(\pi\). Chiaramente questo tipo di algoritmo esegue in modalità \textit{brute force} tutte le possibili soluzioni e restituisce il cammino minimo una volta raccolti tutti i possibili cammini.

\subsubsection{Algoritmo}

Il seguente psuedocodice mostra la funzione HK-INIT utilizzata per l'inizializzazione dei vettori e del tempo iniziale. 
\begin{verbatim}
HK-INIT(G)
    (V,E) = G
    d = NULL
    pi = NULL
    global_time_start = time.now
    return HK-VISIT(0,V)
\end{verbatim}    

Lo pseudocodice della funzione HK-VISIT mostra esattamente i casi base e i casi ricorsivi per la risoluzione del problema in modo ricorsivo sfruttando l'algoritmo di Held and Karp.
\begin{verbatim}
HK-VISIT(v,S)
    // Caso base 1: ritorno il peso dell'arco {v,0}
    if |S| = 1 and S = {v} then 
        return w[v,0]
    // Caso base 2: se la distanza è già stata calcolata, ritorno peso dell'arco
    else if d[v,S] is not empty then 
        return d[v,S]
    // Caso ricorsivo: cerco il cammino minimo tra tutti i sottocammini
    else
        mindist = inf
        minprec = NULL
        for all u in S \ {v} do
            if time.now - global_time_start < 180 then
                dist = HK-VISIT(u,S \ {v})
                if dist + w[u, v] < mindist then
                    mindist = dist + w[u, v]
                    minprec = u
                end if
            else
                return mindist
            end if
        end for
        d[v,S] = mindist
        pi[v,S] = minprec
        return mindist
    end if
\end{verbatim}


\subsubsection{Implementazione}

L'implementazione dell'algoritmo si è basata a partire da una visita in profondità (dall'alto) dei nodi mediante l'uso di due funzioni principali \texttt{hk\_init} e \texttt{hk\_visit}. Queste due funzioni sono contenute dentro una classe Python chiamata \texttt{HeldKarp} che contiene delle variabili per i vettori \(d\) e \(\pi\), nonché per il tempo di esecuzione di inizio usato per bloccare l'algoritmo se il tempo totale supera i tre minuti, come richiesto dalla consegna del progetto.

Al fine di realizzare i vettori \(d\) e \(\pi\) si è deciso di utilizzare la struttura dati \texttt{defaultdict} in modo analogo a una mappa una chiave con un suo valore relativo. Questo è stato particolarmente utile per inserire delle chiavi complesse formate da una stringa generata la seguente struttura: 

\[ v [ p \dots q ]\]

dove \(v\) rappresenta il nodo corrente, mentre \(p\) e \(q\) indicano rispettivamente il nodo iniziale e finale visitato in \(S\), tale per cui \(p \in S\) e \(q \in S\).

Per quanto concerne la complessità, si può affermare che, sebbene l'approccio originale dell'algoritmo porta a concludere la risoluzione dell'intero problema seguendo un approccio "\textit{brute force}", il caso peggiore è certamente inferiore rispetto \(O(n!)\). 
Nello specifico, analizzando i singoli valori:

\begin{itemize}
    \item \(O(n)\) relativo alle \(n-1\) iterazioni richieste per il ciclo che itera tutti i vertici in \(S \ {v}\);
    \item \(O(n\cdot2^n)\) relativo alle coppie usate per l'indicizzazione con \(v\) e \(S\) che sono rispettivamente al più \(n\) e \(2^n\) nel vettore \(d\).
\end{itemize}
Pertanto in totale la complessità può essere riassunta come \(O(n) \cdot O(n\cdot2^n)\), ossia \(O(n^2\cdot2^n)\).

\subsubsection{Analisi della qualità della soluzione}

La soluzione riportata risulta essere esatta dal momento che questo tipo di algoritmo utilizza la programmazione dinamica con un approccio \textit{brute force} per calcolarsi tutte le possibili soluzioni. Per evitare un grande utilizzo di memoria dovuto alle ricorsioni, l'algoritmo è stato bloccato allo scadere di 3 minuti di computazione così da evitare una ricerca troppo lunga.
Inoltre, si è dovuto estendere il limite di ricorsione fino a 5000 chiamate ricorsive di profondità per limitazioni dovute al linguaggio di programmazione Python.

\subsubsection{Analisi dei fattori di approssimazione}

Nell'algoritmo non ci sono state particolari approssimazioni se non nei risultati dei dataset che non hanno completato per intero la risoluzione dei sottoproblemi. Infatti, allo scadere dei 3 minuti i risultati ottenuti in output sono pari alle soluzioni dei cammini minimi fino ad allora trovate. Una giusta approssimazione pertanto non può essere determinata, se non mettendo in relazione l'effettiva soluzione ottima con la soluzione attualmente presa in carico, determinando manualmente e in modo dinamico il tempo richiesto per il raggiungimento dell'effettiva soluzione. 


\subsubsection{Ottimizzazioni implementate}

Nell'algoritmo ci sono state piccole ottimizzazioni a livello di codice di programmazione che hanno permesso di evitare delle operazioni computazionalmente semplici ma che con la complessità analizzata aumentavano di molto il tempo necessario alla computazione. Per prima cosa, l'uso di variabili temporanee ha permesso il riutilizzo di valori precedentemente trovati evitando di doverne ri-eseguire il ricalcolo, come nel caso della generazione della chiave. Nello specifico, la chiave viene generata all'inizio dell'algoritmo a partire dalla conversione in stringa del nodo \(v\) corrente concatenato alla lista \(S\), a sua volta convertita in stringa.
Ci si è accorti che l'uso di una funzione che potesse generare questo tipo di stringa avrebbe aumentato di molto il tempo richiesto per la risoluzione del problema, mentre l'uso di una variabile iniziale ha ridotto di molto il tempo per ricavare il valore della chiave.
Altre soluzioni al posto della chiave interpretata come stringa potevano richiedere l'uso di array per \(S\) che fosse immutabile se usato come chiave, ma si è preferito non indagare ulteriormente visto e considerato che ai fini dell'esercizio una stringa ci è risultata più semplice da indicizzare rispetto a un'intera struttura dati. 

% TODO: ricontrollare che non ci sia scritto una baggianata nei calcoli della complessità
Secondariamente, una seconda ottimizzazione è stata fatta prima di arrivare alla guardia del primo ciclo dove è necessario ricavare un vettore \(S - {v}\) che richiede inevitabilmente la copia profonda e la rimozione di \(v\) con complessità \(O(n) + O(n-1) = O(n)\). Al fine di evitare l'uso della libreria di copia profonda, si è pensato trivialmente di eseguire un ciclo di cui copiare i valori di \(S\) evitando \(v\) e ottenendo pertanto una complessità di \(O(n-1) = O(n)\). 
Contrariamente a quanto si possa pensare, sebbene sulla teoria la complessità è la medesima, il valore ricavato ottimizza di molto le istruzioni effettive necessarie per attuare una copia di questo tipo. A titolo esemplificativo e puramente informale si è rilevato nel caso del dataset \textit{burma14} un decremento del 35\% del tempo impiegato per l'esecuzione sfruttando un ciclo di copia con un \textit{if statement} al posto della copia profonda e della successiva rimozione. 


\subsection{Nearest Neighbor}

\subsubsection{Introduzione}

L'algoritmo \textit{Nearest Neighbor} per il problema del Traveling Salesman Problem rientra nelle cosidette \textit{euristiche costruttive}; queste sono una famiglia di algoritmi che, a partire da un vertice iniziale, procedono aggiungendo un vertice alla volta al sottoinsieme corrispondente alla soluzione parziale. Questa aggiunta viene eseguita secondo regole prefissate, fino all'individuazione di una soluzione approssimata.

\subsubsection{Algoritmo}

Come tutte le euristiche costruttive per il problema TSP, l'algoritmo \textit{nearest neighbor} può essere scomposto in tre passi: inizializzazione, selezione e inserimento. Viene anzitutto illustrato l'algoritmo in pseudocodice, e a seguire vengono analizzati nello specifico i tre passi.

\begin{verbatim}

    NearestNeighbor(G=(V,E))
    # il cammino iniziale è composto unicamente dal primo nodo del grafo
    path <- v1 in V

    # si itera finché tutti i nodi del grafo non sono stati inseriti nel path
    while path != V 
        # si seleziona il nodo NON presente in path di distanza minima dall'ultimo
        # nodo inserito nel path
        nextNode <- MinAdj(path, path.lastV)

        # si inserisce il nodo successivo nel path
        path <- nextNode
    
    # si aggiunge infine il nodo di partenza per chiudere il ciclo
    path <- path + v1

    return path
\end{verbatim}

Le tre fasi, riconoscibili dalla struttura dell'algoritmo, sono:
\begin{enumerate}
    \item \textbf{Inizializzazione}: si parte con il cammino composto unicamente dal primo vertice $v_0$;
    \item \textbf{Selezione}: sia $(v_0,...,v_k)$ il cammino corrente; il vertice successivo è il vertice $v_{k+1}$, non presente nel cammino, a distanza minima da $v_k$;
    \item \textbf{Inserimento}: viene inserito $v_{k+1}$ subito dopo $v_k$ nel cammino.
\end{enumerate}

I passi 2 e 3 vengono ripetuti finché tutti i vertici non sono nel cammino finale. Una volta verificata questa condizione è necessario eseguire un'ulteriore singola istruzione: aggiungere in coda al percorso finale il nodo iniziale. Questa operazione è necessaria ai fini del mantenimento della proprietà di percorso minimo del problema TSP. \\
Questo algoritmo, come le altre euristiche costruttive, ha complessità computazionale pari a $O(|V|^2)$.

\subsubsection{Analisi dei fattori di approssimazione}

Definita la disuguaglianza triangolare come 
\begin{equation}
    \forall u, v, w \in V, \textnormal{ vale che } c(u, v) \le c(u, w) + c(w, v)    
\end{equation}
l'algoritmo \textit{nearest neighbor} permette di trovare una soluzione $log(n)$-approssimata a TSP quando la disuguaglianza triangolare è rispettata.

\subsubsection{Implementazione}

La nostra implementazione ricalca l'implementazione standard dell'algoritmo Nearest Neighbor per il problema del TSP; essa segue infatti i tre passi sopracitati mantenendo intatta la logica dell'algoritmo di riferimento. \\
Operativamente, il codice dell'algoritmo si divide in una classe, \texttt{NearestNeighbor}, e in alcuni metodi di utilità implementate per la classe \texttt{TSP} citata precedentemente. \\
La classe \texttt{NearestNeighbor} presenta un solo metodo, chiamato \texttt{algorithm}, che si occupa di effettuare l'algoritmo in questione su un singolo grafo di classe \texttt{TSP}; esso riceve in \textit{input} il grafo e restituisce il peso totale del percorso hamiltoniano trovato. \\
Il metodo \texttt{algorithm} chiama, al suo interno, i metodi \texttt{delete\_node} e \texttt{get\_min\_node} della classe \texttt{TSP}; questi sono di particolare importanza nella nostra implementazione, poiché permettono di mantenere la complessità al suo valore teorico, come viene illustrato nella sezione seguente.

\subsubsection{Ottimizzazioni implementate}

La principale ottimizzazione implementata risiede nel metodo con il quale viene selezionato il nodo successivo da inserire nel cammino finale. Un'implementazione \textit{naive} dell'algoritmo, infatti, potrebbe ricercare il nodo da inserire scorrendo linearmente l'intero grafo e controllando, per ogni nodo analizzato, che questi non sia già presente nel cammino finale e che sia a distanza minima dall'ultimo nodo inserito nel percorso finale. Questa implementazione, però, è stata giudicata inefficiente, poiché richiederebbe di analizzare ogni nodo anche se è già stato visitato in precedenza. \\
Abbiamo quindi adottato un approccio "inverso": nella fase di inizializzazione dell'algoritmo viene infatti creata una copia (profonda) del grafo iniziale; da questa copia vengono poi, nella fase di selezione, eliminati i nodi già considerati. Questa ottimizzazione permette di effettuare la ricerca esclusivamente sui nodi non ancora considerati, il cui numero diminuisce costantemente durante l'iterazione della fase di selezione. \\
A questa ottimizzazione si aggiunge l'utilizzo di una semplice lista, chiamata \textit{visited}, che permette di mantenere l'integrità dell'algoritmo nel momento in cui viene chiamata la funzione \texttt{get\_min\_node}: se un nodo è già stato visitato, il controllo sul peso dell'arco che lo unisce al nodo in analisi non viene neanche effettuato. \\
Un'ultima ottimizzazione risiede nella funzione \texttt{get\_min\_node} di \texttt{TSP}, che fa uso delle liste di adiacenza per la ricerca del nodo di peso minimo; in questo modo, invece di calcolare a \textit{runtime} il peso tra un nodo e l'altro, è sufficiente uno scorrimento lineare della matrice di adiacenza sulla riga (o sulla colonna) relativa al nodo in esame.

\subsection{2-approssimato}

\subsubsection{Introduzione}

L'algoritmo 2-approssimato è in grado di determinare un'approssimazione del TSP sotto la condizione che le
distanze rispettino la \textit{disuguaglianza triangolare}, ovvero:
\begin{equation}
    \forall u, v, w \in V, \textnormal{ vale che } c(u, v) \le c(u, w) + c(w, v)
\end{equation}

dove $c$ è la \textit{funzione di costo}. Questo implica che il cammino con un lato $c(u, v)$ ha un costo
minore o uguale al costo del cammino con due lati $c(u, w, v)$. Il nome di questo problema è \textbf{TRIANGLE\_TSP}.

\subsubsection{Definizione di MST}

Sia $V$ l'insieme dei nodi che costituiscono il grafo pesato $G$ e sia $E$ la collezione dei lati di tale
grafo. Ai fini delle analisi della complessità degli algoritmi, sia $|V| = n$ e $|E| = m$.

Un \textit{minimum spanning tree} è un
sottoinsieme dei lati $E$ di un grafo $G$ non orientato connesso e pesato sui lati che
collega tutti i vertici insieme, senza alcun ciclo e con il minimo peso totale del
lato possibile. Cioè, è uno spanning tree la cui somma dei pesi dei bordi è la più
piccola possibile.

Un \textit{minimum spanning tree} $T = (V, E')$ è un albero, il cui insieme dei lati $E'$ è un
sottoinsieme dei lati $E$ di un grafo $G = (V, E)$ non orientato, connesso e
pesato, che collega tutti i vertici $V$, la cui somma dei pesi dei lati è la minima.

L'algoritmo generico per determinare un MST è:
\begin{verbatim}
    A = empty_set
    while A doesn't form a spanning tree
        find an edge (u,v) that is safe for A
        A = A U {(u,v)}
    return A
\end{verbatim}

Si forniscono alcune definizioni per gli MST:
\begin{enumerate}
    \item un \textbf{taglio} $(S, V \setminus S)$ di un grafo $G = (V, E)$ è una partizione
    di $V$;
    \item un lato $(u, v) \in E$ \textbf{attraversa il taglio} $(S, V \setminus S)$ se
    $u \in S$ e $v \in V \setminus S$ o viceversa;
    \item un taglio \textbf{rispetta} un insieme $A$ di lati se nessun lato di $A$ attraversa
    il taglio;
    \item dato un taglio, il lato che lo attraversa di peso minimo si chiama \textbf{light edge}.
\end{enumerate}

Per determinare se un lato è \textbf{safe}, si sfrutta il seguente teorema:

\textbf{Teorema}: Sia $G = (V, E)$ un grafo non diretto, connesso e pesato. Sia $A$ un
sottoinsieme di $E$ incluso in una qualche MST di $G$, sia $(S, V \setminus S)$ un
taglio che rispetta $A$, e sia $(u, v)$ un \textit{light edge} per $(S, V \setminus S)$.
Allora $(u, v)$ è \textit{safe} per $A$.

\subsubsection{Algoritmo}

L'idea che sta dietro a questo algoritmo consiste nell'utilizzare un algoritmo per il calcolo
del MST. Tuttavia, il MST è un albero e quello che vogliamo ottenere invece è un
ciclo hamiltoniano. Per fare ciò, eseguiamo una visita in \textit{preorder} del MST
e aggiungiamo la radice di tale MST alla lista della determinata dalla preorder. Questo è un ciclo
hamiltoniano del grafo originale, in quanto quest'ultimo è completo. Pertanto, esiste sempre un
lato tra ogni coppia di vertici.

\begin{verbatim}
    2-APPROXIMATION(G=(V,E), c)
        root <- v1 in V         // scelgo un nodo di V come radice per Prim
        T <- Prim(G, c, root)
        H' <- preorder(root)    // visita in preorder
        H <- H U {root}           // aggiungo il percorso che va dalll'ultimo nodo
                                // della visita in preorder alla radice
        return H

\end{verbatim}

\subsubsection{Analisi della qualità della soluzione}

Si illustri l'analisi della qualità della soluzione ritornata dall'algoritmo:
\begin{enumerate}
    \item Il costo di $H'$ è basso per la definizione di MST;
    \item supponiamo che presi due vertici $a$ e $b$ non siano collegati da un lato (anche se
    sappiamo che il grafo è completo), L'idea è che il costo di $(a, b)$ è minore rispetto
    al costo di fare un giro più largo, in quanto vale la \textit{disuguaglianza triangolare}.
    Quindi in questo caso $(a, b)$ è un \textit{shortcut}.
\end{enumerate}

\subsubsection{Analisi del fattore di approssimazione}
Si illustri l'analisi del fattore di approssimazione dell'algoritmo:
\begin{enumerate}
    \item \textit{Limite inferiore al costo della soluzione ottima $H$}: sia $H$ il ciclo
    ottimo $H^{ottimo} = <v_{j1}, ..., v_{jn}, v_{j1}>$.
    Sia $H'^{ottimo} = <v_{j1}, ..., v_{jn}>$ un cammino
    (è uno spanning tree, non un ciclo) hamiltoniano. Quindi, $c(H') \ge c(T)$, dove $T$ è
    il MST, e $c(H^{ottimo}) \ge c(H'^{ottimo})$ prechè i costi sono maggiori o uguali a zero.

    \item \textit{Limite superiore al costo della soluzione restituita $H$}: dato un albero,
    una \textit{full preorder chain} è una lista con ripetizioni dei nodi dell'albero che
    indica i nodi raggiunti dalle chiamate ricorsive dell'algoritmo \verb|Preorder|.

    \textit{Proprietà}: in una full preorder chain ogni arco di $T$ appare esattamente due
    volte. Quindi $c(\textnormal{full preorder chain}) = 2 \cdot c(T)$. Se si eliminano
    dalla full preorder chain tutte le occorrenze successive alla prima dei nodi interni
    (tranne l'ultima occorrenza della radice) otteniamo, grazie alla
    \textit{disuguaglianza triangolare}:
    \[
        2 \cdot c(T) \ge c(H) \Rightarrow 2 \cdot c(H^{ottimo}) \ge 2 \cdot c(T) \ge c(H)
        \Rightarrow \frac{c(H)}{c(H^{ottimo})} \le 2
    \]

\end{enumerate}

\subsubsection{Implementazione}

\subsubsection{Ottimizzazioni implementate}

