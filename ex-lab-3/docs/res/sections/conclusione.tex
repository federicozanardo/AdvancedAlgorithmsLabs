\section{Conclusione}

Alla fine del lavoro svolto possiamo affermare che i risultati ottenuti sono stati più che soddisfacenti: per entrambi gli algoritmi, infatti, la complessità rilevata è quasi perfettamente aderente alla complessità teorica attesa. \\
Per quanto riguarda le ottimizzazioni implementate, sia per l'algoritmo di Stoer e Wagner, che per l'algoritmo di Karger e Stein abbiamo riscontrato un netto miglioramento rispetto a una prima implementazione \textit{naive}, i quali effettuavano delle copie profonde delle struttura dati numerose volte, ed erano perciò inefficiente.\\
Per quanto riguarda Stoer e Wagner, si è rivelato inoltre particolarmente utile l'utilizzo della struttura dati \textit{MaxHeap} per l'estrazione del nodo di peso maggiore da parte della \textit{subroutine} \texttt{ExtractMax} nella funzione \texttt{stMinCut}; essendo questa operazione eseguita numerose volte è infatti risultato vantaggioso avere una funzione che estraesse il massimo in tempo logaritmico (invece che lineare). \\
Avremmo potuto implementare lo \textit{heap} come \textit{Fibonacci Heap}; abbiamo però considerato che, avendo le istanze di input un numero di nodi e di archi dello stesso ordine di grandezza (con un numero di archi di poco superiore al numero di vertici), il vantaggio in termini temporali sarebbe stato difficilmente apprezzabile. Abbiamo quindi preferito risparmiare del tempo per poter meglio lavorare sulle implementazioni dei due algoritmi, riadattando semplicemente la struttura dati a partire dall'implementazione fatta per un laboratorio precedente.\\
Parlando di precisione delle misurazioni, abbiamo deciso di adottare anche per questo laboratorio le misurazioni multiple per dataset la cui computazione avveniva in tempi inferiori al secondo: in questo modo abbiamo potuto misurare con più precisione il tempo di esecuzione degli algoritmi.

Per quanto concerne il workflow, siamo riusciti a lavorare bene e con una buona sincronia su entrambi gli algoritmi sfruttando ancora una volta Git (e nello specifico la piattaforma Github) per il versionamento dei file Python e per la presente relazione. Inoltre, l'uso degli strumenti di collaborazione con VS Code ci ha aiutato a collaborare in remoto, abbattendo così i tempi per il confronto e lo sviluppo collaborativo sugli algoritmi. Per alcune strutture dati abbiamo inoltre svolto un paio di test di unità così da verificarne la correttezza in corso d'opera per il \textit{max-heap} e per la struttura dati \textit{graph}. 

Concludendo, riteniamo che il lavoro sia stato svolto in modo collaborativo nel migliore dei modi, sfruttando tutti gli strumenti e le conoscenze a nostra disposizione. Anche per quanto riguarda i risultati, le conclusioni in merito all'algoritmo migliore sono evidenti e l'implementazione degli algoritmi sfruttando Python non ci ha portato troppi problemi rispetto agli altri laboratori, visto e considerato che buona parte di queste strutture dati le abbiamo potute riutilizzare riadattando ove necessario il codice. 
